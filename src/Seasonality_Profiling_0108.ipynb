{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.cluster import MeanShift\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_wd = os.getcwd()\n",
    "data_dir = os.path.join(curr_wd, 'Data')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demand_raw_dir = os.path.join(data_dir, 'SAMPLE3YearSales.txt')\n",
    "demand_raw_df = pd.read_csv(demand_raw_dir, sep = '\\t', header=0)\n",
    "print (np.size(demand_raw_df, 0))\n",
    "demand_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_raw_demand_dir = os.path.join(data_dir, 'SAMPLEWeek1_4.txt')\n",
    "add_raw_demand_df = pd.read_csv(add_raw_demand_dir, sep='\\t', header=0)\n",
    "add_raw_demand_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_add = add_raw_demand_df.sort_values(['Subclass', 'SKU', 'Store', 'Week'])\n",
    "LIST = format_add.loc[:, ['SKU', 'Store']].drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "add_sales = format_add[(format_add.Week >= 201701)]['Units'].values.reshape(-1,add_raw_demand_df.Week.nunique())\n",
    "add_sales_df = pd.DataFrame(add_sales, columns = [i for i in range(1, add_raw_demand_df.Week.nunique()+1)])\n",
    "\n",
    "add_df = pd.concat([LIST, add_sales_df], axis = 1)\n",
    "add_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_df = demand_raw_df.sort_values(['Subclass', 'SKU', 'Store', 'Week'])\n",
    "STR_SKU_LIST = format_df.loc[:, ['SKU', 'Store']].drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "sales_2014 = format_df[(format_df.Week < 201501)]['Units'].values.reshape(-1,52)\n",
    "sales_2015 = format_df[(format_df.Week > 201452) & (format_df.Week < 201601)]['Units'].values.reshape(-1,52)\n",
    "sales_2016 = format_df[(format_df.Week > 201552) & (format_df.Week < 201701)]['Units'].values.reshape(-1,52)\n",
    "\n",
    "two_years_mean = (sales_2014 + sales_2015) / 2\n",
    "three_years_mean = (sales_2014 + sales_2015 + sales_2016) / 3\n",
    "\n",
    "df_2Years = pd.DataFrame(two_years_mean, columns = [i for i in range(1,53)])\n",
    "df_3Years = pd.DataFrame(three_years_mean, columns = [i for i in range(1,53)])\n",
    "\n",
    "df_Year1 = pd.DataFrame(sales_2014, columns = [i for i in range(1,53)])\n",
    "df_Year2 = pd.DataFrame(sales_2015, columns = [i for i in range(1,53)])\n",
    "df_Year3 = pd.DataFrame(sales_2016, columns = [i for i in range(1,53)])\n",
    "\n",
    "two_years_mean_df = pd.concat([STR_SKU_LIST, df_2Years], axis = 1)\n",
    "three_years_mean_df = pd.concat([STR_SKU_LIST, df_3Years], axis = 1)\n",
    "year1_df = pd.concat([STR_SKU_LIST, df_Year1], axis = 1)\n",
    "year2_df = pd.concat([STR_SKU_LIST, df_Year2], axis = 1)\n",
    "year3_df = pd.concat([STR_SKU_LIST, df_Year3], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions:\n",
    "\n",
    "def calc_ncq(df):\n",
    "    ncq_df = df.iloc[:,2:54].cumsum(axis = 1)\n",
    "    ncq_max = ncq_df.iloc[:,-1]\n",
    "    ncq_out =  ncq_df.values /  ncq_max.values[:,None]\n",
    "    out_df = df.copy()\n",
    "    out_df.iloc[:,2:54] = ncq_out\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "def scal_qty(df):\n",
    "    scal_df = df.iloc[:,2:54].copy()\n",
    "    scal_mean = scal_df.mean(axis = 1)\n",
    "    scal_out =  scal_df.values /  scal_mean.values[:,None]\n",
    "    out_df = df.copy()\n",
    "    out_df.iloc[:,2:54] = scal_out\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "def get_km_result_df(data_ncq, data_scal, kmeans):\n",
    "    title_df = data_ncq.loc[:,['SKU', 'Store']]\n",
    "    title_df['Class_label'] = kmeans.labels_\n",
    "    scal_df = title_df.merge(data_scal, how = 'left', on =['SKU', 'Store'])\n",
    "    centers_df = scal_df.iloc[:,2:].groupby('Class_label').mean().copy()\n",
    "    centers_df.reset_index(inplace=True)\n",
    "    kmean_result_df = title_df.merge(centers_df, how = 'left', on =['Class_label'])\n",
    "    \n",
    "    return kmean_result_df\n",
    "\n",
    "def scal_error_est(km_result, year3_df):\n",
    "    year3_scal = scal_qty(year3_df)\n",
    "    compare = km_result.merge(year3_scal, how = 'left', on =['SKU', 'Store'])\n",
    "    error_matrix = compare.iloc[:,3:55].values - compare.iloc[:,55:107].values\n",
    "    \n",
    "    squared_error = np.sum(np.square(error_matrix))\n",
    "    return squared_error\n",
    "\n",
    "def ncq_error_est(km_result, year3_df):\n",
    "    year3_scal = calc_ncq(year3_df)\n",
    "    km = calc_ncq(km_result.drop(['Class_label'], axis = 1))\n",
    "    compare = km.merge(year3_scal, how = 'left', on =['SKU', 'Store'])\n",
    "    error_matrix = compare.iloc[:,2:54].values - compare.iloc[:,54:106].values\n",
    "    \n",
    "    squared_error = np.sum(np.square(error_matrix))\n",
    "    return squared_error\n",
    "\n",
    "def get_Kmeans_fcst(base_df, factors_df):\n",
    "    merge = pd.merge(base_df, factors_df.drop(['Class_label'], axis=1), how='inner', on=['SKU', 'Store'])\n",
    "    matrix_base = merge.iloc[:,2:54].values\n",
    "    matrix_factors = merge.iloc[:,54:106].values\n",
    "    matrix_fcst = matrix_base*matrix_factors\n",
    "    fcst_df = pd.DataFrame(matrix_fcst, columns=['Wk%d' % i for i in range(1,53)])\n",
    "    result = pd.concat([merge.loc[:,['SKU','Store']], fcst_df], axis =1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to find the best K\n",
    "\n",
    "#drop rows with any 'null' value \n",
    "#drop rows with sum of all sales equal to zero\n",
    "data_clean = two_years_mean_df.dropna(axis = 0, how = 'any')\n",
    "data_clean = data_clean[(data_clean.iloc[:,2:54].sum(axis =1) > 0)]\n",
    "\n",
    "#smooth raw demand using exponential weighted moving average\n",
    "data_ewma = data_clean.copy()\n",
    "data_ewma.iloc[:,2:54] = data_clean.iloc[:,2:54].ewm(span = 4, axis = 1).mean()\n",
    "print('Count of Store_SKU in ewma: %d' % np.size(data_ewma, 0))\n",
    "\n",
    "#set threshhold for sales volume, item will be grouped together when annully sales <= 52\n",
    "data_slow_mover = data_ewma[(data_ewma.iloc[:,2:54].sum(axis =1) <= 52)]\n",
    "data_slow_mover = data_slow_mover.iloc[:, 0:54]\n",
    "print ('Count of Store_SKU in slow_mover: %d' % np.size(data_slow_mover, 0))\n",
    "\n",
    "#set threshhold for sales volume, item can be processed when annully sales > 52\n",
    "data_vol = data_ewma[(data_ewma.iloc[:,2:54].sum(axis =1) > 52)]\n",
    "print ('Count of Store_SKU to be processed: %d' % np.size(data_vol, 0))\n",
    "\n",
    "\n",
    "data_ncq = calc_ncq(data_vol)\n",
    "data_scal = scal_qty(data_vol)\n",
    "\n",
    "\n",
    "#Finding best K for target dataset\n",
    "test_list =[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "squared_error = []\n",
    "for i in test_list:\n",
    "    print (str(datetime.datetime.now()))\n",
    "    X_train = data_ncq.iloc[:,2:54].values\n",
    "    kmeans = KMeans(n_clusters = i, random_state=0).fit(X_train)\n",
    "    error = ncq_error_est(get_km_result_df(data_ncq, data_scal, kmeans), year3_df)\n",
    "    squared_error.append(error)\n",
    "    print (str(datetime.datetime.now()))\n",
    "    print (\"Error for %d is: %s\" % (i, error))\n",
    "\n",
    "print(squared_error)\n",
    "\n",
    "\n",
    "plt.plot(test_list, squared_error)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_k = test_list[squared_error.index(min(squared_error))]\n",
    "final_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final run with best k\n",
    "\n",
    "\n",
    "#drop rows with any 'null' value \n",
    "#drop rows with sum of all sales equal to zero\n",
    "data_clean = three_years_mean_df.dropna(axis = 0, how = 'any')\n",
    "data_clean = data_clean[(data_clean.iloc[:,2:54].sum(axis =1) > 0)]\n",
    "\n",
    "#smooth raw demand using exponential weighted moving average\n",
    "data_ewma = data_clean.copy()\n",
    "data_ewma.iloc[:,2:54] = data_clean.iloc[:,2:54].ewm(span = 4, axis = 1).mean()\n",
    "print('Count of Store_SKU in ewma: %d' % np.size(data_ewma, 0))\n",
    "\n",
    "#set threshhold for sales volume, item will be grouped together when annully sales <= 52\n",
    "data_slow_mover = data_ewma[(data_ewma.iloc[:,2:54].sum(axis =1) <= 52)]\n",
    "data_slow_mover = data_slow_mover.iloc[:, 0:54]\n",
    "print ('Count of Store_SKU in slow_mover: %d' % np.size(data_slow_mover, 0))\n",
    "\n",
    "#set threshhold for sales volume, item can be processed when annully sales > 52\n",
    "data_vol = data_ewma[(data_ewma.iloc[:,2:54].sum(axis =1) > 52)]\n",
    "print ('Count of Store_SKU to be processed: %d' % np.size(data_vol, 0))\n",
    "\n",
    "\n",
    "data_ncq = calc_ncq(data_vol)\n",
    "data_scal = scal_qty(data_vol)\n",
    "\n",
    "\n",
    "print (str(datetime.datetime.now()))\n",
    "X_train = data_ncq.iloc[:,2:54].values\n",
    "kmeans = KMeans(n_clusters = final_k, random_state=0).fit(X_train)\n",
    "\n",
    "pieces = []\n",
    "\n",
    "#get the result from Kmeans\n",
    "kmeans_result = get_km_result_df(data_ncq, data_scal, kmeans)\n",
    "pieces.append(kmeans_result)\n",
    "\n",
    "\n",
    "#get labels for slow movers\n",
    "scal_slow_mover = scal_qty(data_slow_mover)\n",
    "scal_slow_mover['Class_label'] = np.array([9999]*np.size(scal_slow_mover, 0))\n",
    "slow_mover_centers = scal_slow_mover.iloc[:,2:].groupby('Class_label').mean()\n",
    "slow_mover_centers.reset_index(inplace=True)\n",
    "slow_mover_result = scal_slow_mover.loc[:,['SKU', 'Store', 'Class_label']].merge(slow_mover_centers, how='left', on=['Class_label'])\n",
    "\n",
    "pieces.append(slow_mover_result)\n",
    "\n",
    "df_result = pd.concat(pieces)\n",
    "df_result['SKU'] = df_result['SKU'].astype('object')\n",
    "df_result['Store'] = df_result['Store'].astype('object')\n",
    "\n",
    "print (str(datetime.datetime.now()))\n",
    "\n",
    "df_result.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export seasonal factors at str_sku level\n",
    "df_export = df_result.copy()\n",
    "df_export.to_csv(os.path.join(data_dir, 'Output/sample_Seasonalities_0109.csv'))\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_export[(df_export.SKU == 999) & (df_export.Store == 102)].iloc[:,3:].values\n",
    "plt.plot(a[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get ARIMA output\n",
    "\n",
    "ARIMA_dir = os.path.join(data_dir, 'fake_ARIMA_0109.txt')\n",
    "ARIMA_df_raw = pd.read_csv(ARIMA_dir, sep = ',', header = 0, index_col=0).sort_values(by=['SKU', 'Store', 'Week'])\n",
    "ARIMA_df_raw.reset_index(drop=True)\n",
    "STR_SKU_LIST = ARIMA_df_raw.loc[:, ['SKU', 'Store']].drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "ARIMA_output = ARIMA_df_raw['Units'].values.reshape(-1,52)\n",
    "ARIMA_h = pd.DataFrame(ARIMA_output, columns = [i for i in range(1,53)])\n",
    "ARIMA_df = pd.concat([STR_SKU_LIST, ARIMA_h], axis=1)\n",
    "ARIMA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_fcst = get_Kmeans_fcst(ARIMA_df, df_result)\n",
    "# kmean_fcst.to_csv(os.path.join(data_dir, 'Output/final_forecast_0109.csv'))\n",
    "kmean_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sls(df, *num_plot):\n",
    "    if not num_plot:\n",
    "        for i in range(np.size(df, 0)):\n",
    "            plt.plot(df.iloc[i, 2:].values)\n",
    "    else:\n",
    "        rand_int = np.random.randint(0, np.size(df, 0), num_plot[0])\n",
    "        plot_df = df.iloc[rand_int, :]\n",
    "        plot_data = plot_df.iloc[:, 2:]\n",
    "        item_loc = plot_df.iloc[:,0:2]\n",
    "\n",
    "        for i in range(num_plot[0]):\n",
    "            plt.plot(plot_data.iloc[i].values)\n",
    "    \n",
    "    ticks = np.arange(0, 52, 5)\n",
    "    labels = np.arange(1, 53, 5)\n",
    "    plt.xticks(ticks, labels)\n",
    "    print (item_loc)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_sales_fcst(SKU, Store):\n",
    "    sales_2014 = year1_df[(year1_df.SKU == SKU) & (year1_df.Store == Store)].iloc[:,2:]\n",
    "    sales_2015 = year2_df[(year2_df.SKU == SKU) & (year2_df.Store == Store)].iloc[:,2:]\n",
    "    sales_2016 = year3_df[(year3_df.SKU == SKU) & (year3_df.Store == Store)].iloc[:,2:]\n",
    "    fcst = kmean_fcst[(kmean_fcst.SKU == SKU) & (kmean_fcst.Store == Store)].iloc[:,2:]\n",
    "    \n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sales2014, = plt.plot(sales_2014.iloc[0].values, label = 'sales2014')\n",
    "    sales2015, = plt.plot(sales_2015.iloc[0].values, label = 'sales2015')\n",
    "    sales2016, = plt.plot(sales_2016.iloc[0].values, label = 'sales2016')\n",
    "    fcst, = plt.plot(fcst.iloc[0].values, label = 'fcst')\n",
    "    ticks = np.arange(0, 52, 5)\n",
    "    labels = np.arange(1, 53, 5)\n",
    "    plt.xticks(ticks, labels)\n",
    "    plt.legend(handles=[sales2014, sales2015, sales2016, fcst])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot sales hist and fcst for a SKU, Store\n",
    "plot_sales_fcst(999, 109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sls(kmean_fcst,2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
